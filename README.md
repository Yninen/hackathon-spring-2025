# hackathon-spring-2025. Команда "Нейроэмоции".


## Исследование и лингвистическая обработка

* **Загрузка данных:** Код может загружать данные из нескольких CSV-файлов, предоставленных компанией "Норси-Транс", и объединять их в один датафрейм Pandas.

* **Первичный анализ данных:** Код отображает общую информацию о загруженных данных, включая количество записей, первые несколько строк, информацию о типах данных и количестве пропущенных значений, а также количество пустых строк в каждой текстовой колонке.

* **Анализ длины текста:** Код вычисляет и визуализирует распределение длины текста в колонках 'doc_text', 'image2text' и 'speech2text' с помощью гистограмм.

* **Очистка текста:** Реализована функция clean_text, которая выполняет следующие действия:

	1. Удаление URL-адресов.
	2. Удаление упоминаний доменов (.com, .ru, .net, .org).
	3. Удаление специальных символов и эмодзи.
	4. Удаление лишних пробелов. Эта функция применяется к колонкам 'doc_text', 'image2text' и 'speech2text', создавая новые колонки с очищенным текстом ('doc_text_clean', 'image2text_clean', 'speech2text_clean'). Также удаляются полностью пустые строки после очистки.

* **Удаление дубликатов:** Код обнаруживает и удаляет дубликаты на основе очищенного текста в каждой из трех текстовых колонок.

* **Предварительная обработка текста с использованием библиотеки Natasha:** Для дальнейшей обработки текста используется библиотека Natasha Процесс включает в себя:

	1. Сегментацию: Разбиение текста на токены (слова).
	2. Морфологический анализ: Определение морфологических характеристик каждого токена.
	3. Лемматизацию: Приведение слов к их базовой форме (лемме).
	4. Удаление стоп-слов: Фильтрация распространенных русских слов и специфичных для социальных сетей стоп-слов. Результаты предварительной обработки сохраняются в новых колонках ('doc_text_processed', 'image2text_processed', 'speech2text_processed'). Также удаляются строки, где все предобработанные текстовые колонки пусты.

* **Анализ частоты слов:** Код вычисляет частоту встречаемости каждого слова в предобработанных текстовых колонках и выводит топ-30 наиболее часто встречающихся слов для каждой из них. Также строятся графики (гистограммы и круговые диаграммы) для визуализации частоты топ-30 и топ-10 слов соответственно. Проводится и визуализация объединенной статистики частоты слов из всех источников.

* **Сохранение обработанных данных:** Исходные и обработанные текстовые данные сохраняются в новый CSV-файл 'NeuroEmotions_data.csv'.


## Тест моделей

На данном этапе протестировано несколько моделей:

1. Word2Vec.
2. TFIDF.
3. Трансформер от Сбера 'sberbank-ai/sbert_large_nlu_ru'.
4. Модель'paraphrase-multilingual-mpnet-base-v2'.

На данный момент модель от Сбера показала лучший результат. Рассмотрим основную функцию модели - 'semantic_search'.

#### Этап 1. Подготовка кандидатов

1.  **Токенизация документа**:
    
    -   Регулярное выражение  `r'\w+'`  выделяет все слова из текста
        
    -   Для каждого слова сохраняется:
        -   Текст слова 
        -   Позиции начала и конца в исходном документе
            
2.  **Генерация N-грамм**:
    
    -   Создаются униграммы (отдельные слова)        
    -   Биграммы (пары соседних слов)     
    -   Триграммы (тройки соседних слов)    
    -   Для каждой N-граммы сохраняются объединенный текст и позиции
        
#### Этап 2. Семантическое сравнение

1.  **Кодирование в эмбеддинги**:
    
    -   Запрос и все кандидаты преобразуются в векторные представления с помощью SBERT
        
2.  **Расчет схожести**:    

    -   Вычисляется косинусная схожесть между вектором запроса и векторами всех кандидатов        
    -   Находится кандидат с максимальной схожестью
        

#### Этап 3. Проверка порога и возврат результата

-   Если максимальная схожесть ≥ threshold (0.65 по умолчанию):    
    -   Возвращается лучший кандидат, его позиции и вероятность        
-   Иначе:    
    -   Выводится предупреждение и возвращается  `None`
 


Таким образом, на текущем этапе исследован и обработан датасет, протестировано несколько моделей для семантического анализа. Дальше планируется провести разработку CLI интерфейса, обучение модели на собственных данных, оптимизацию модели, упаковку через Docker и подготовку документации, оформление финальных материалов.
